{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5zJ05RPTCon",
   "metadata": {
    "id": "d5zJ05RPTCon"
   },
   "source": [
    "# Inference\n",
    "\n",
    "This notebook is made to load the existing model parameters, take a checkbox image as an input and predict whether that checkbox is checked, uchecked or not a checkbox at all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78m8Nh-fTyZn",
   "metadata": {
    "id": "78m8Nh-fTyZn"
   },
   "source": [
    "## Recreating the model and loading the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820b576a-cad0-4a14-ba07-fa70126f4f47",
   "metadata": {
    "id": "820b576a-cad0-4a14-ba07-fa70126f4f47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CheckboxClassifier(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=16384, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class CheckboxClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.ReLU()(self.conv1(x)))\n",
    "        x = self.pool(nn.ReLU()(self.conv2(x)))\n",
    "        x = nn.Flatten()(x)\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CheckboxClassifier().to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"checkbox_nn_parameters.pth\", map_location=device))\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WYprs1zOT5M7",
   "metadata": {
    "id": "WYprs1zOT5M7"
   },
   "source": [
    "## Recreating the transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05b64ed7-55bd-4785-b44c-64a1b34b8476",
   "metadata": {
    "id": "05b64ed7-55bd-4785-b44c-64a1b34b8476"
   },
   "outputs": [],
   "source": [
    "class CustomPad:\n",
    "    def __init__(self, padding):\n",
    "        self.padding = padding\n",
    "\n",
    "    def _find_border_color(self, image):\n",
    "        pixels = list(image.getdata())\n",
    "        width, height = image.size\n",
    "        edge_pixels = (\n",
    "            pixels[:width]\n",
    "            + pixels[-width:]\n",
    "            + [pixels[n * width] for n in range(height)]\n",
    "            + [pixels[n * width - 1] for n in range(1, height + 1)]\n",
    "        )\n",
    "        most_common_color = Counter(edge_pixels).most_common(1)[0][0]\n",
    "\n",
    "        return most_common_color\n",
    "\n",
    "    def __call__(self, img):\n",
    "        left = top = right = bottom = self.padding\n",
    "\n",
    "        border_color = self._find_border_color(img)\n",
    "        new_img = ImageOps.expand(\n",
    "            img, border=(left, top, right, bottom), fill=border_color\n",
    "        )\n",
    "        return new_img\n",
    "\n",
    "\n",
    "class LargestContourCrop:\n",
    "    def __init__(self, padding=10, sigma=0.5):\n",
    "        self.padding = padding\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image_np = np.array(image)\n",
    "\n",
    "        gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "        blurred = cv2.GaussianBlur(gray, (0, 0), self.sigma)\n",
    "\n",
    "        edges = cv2.Canny(blurred, threshold1=30, threshold2=100)\n",
    "        contours, _ = cv2.findContours(\n",
    "            edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "\n",
    "        if contours:\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "            x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "\n",
    "            x_pad = max(x - self.padding, 0)\n",
    "            y_pad = max(y - self.padding, 0)\n",
    "            w_pad = min(w + 2 * self.padding, image_np.shape[1] - x_pad)\n",
    "            h_pad = min(h + 2 * self.padding, image_np.shape[0] - y_pad)\n",
    "\n",
    "            cropped_image = image_np[y_pad : y_pad + h_pad, x_pad : x_pad + w_pad]\n",
    "\n",
    "            return Image.fromarray(cropped_image)\n",
    "\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1RS3fssFUArr",
   "metadata": {
    "id": "1RS3fssFUArr"
   },
   "source": [
    "## Inference script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f59e2d4-fdc6-4686-8a8a-5c348ded2624",
   "metadata": {
    "id": "7f59e2d4-fdc6-4686-8a8a-5c348ded2624"
   },
   "outputs": [],
   "source": [
    "def classify_image(image_path, model, device):\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "            CustomPad(padding=20),\n",
    "            LargestContourCrop(padding=10, sigma=0.5),\n",
    "            transforms.Resize((64, 64)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    _, predicted_class = torch.max(probabilities, 1)\n",
    "    probs = probabilities.squeeze().tolist()\n",
    "    checkbox_classes = [\"checked\", \"unchecked\", \"other\"]\n",
    "\n",
    "    return {\n",
    "        \"probs\": {\n",
    "            checkbox_class: prob\n",
    "            for checkbox_class, prob in zip(checkbox_classes, probs)\n",
    "        },\n",
    "        \"predicted_class\": checkbox_classes[predicted_class.item()],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s5P54CMUUI7J",
   "metadata": {
    "id": "s5P54CMUUI7J"
   },
   "source": [
    "## Run the script!\n",
    "\n",
    "Pass an image as an input and get probabilities and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adbe92d4-bf90-402b-a08d-c2fe395c36d1",
   "metadata": {
    "id": "adbe92d4-bf90-402b-a08d-c2fe395c36d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'probs': {'checked': 0.3265308737754822,\n",
       "  'unchecked': 0.6046016812324524,\n",
       "  'other': 0.0688675045967102},\n",
       " 'predicted_class': 'unchecked'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_image(\"checkbox.png\", model, device)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
